#Create the three samples for training, tuning and testing
train= df %>% filter(time %in% 1:initial_T) %>% select(-c(id, time))
val= df %>% filter(time %in% (initial_T+1):(initial_T+val_T)) %>% select(-c(id, time))
test= df %>%
filter( time > initial_T+val_T ) %>%
select(-c(id,time))
full.train = rbind(train,val)
################################################################################
## ------ ESTIMATING MODELS FOR THE GIVEN TRAINING AND VAL SAMPLE --------
################################################################################
### ------------------------------------------------------------------
# Standard Linear Model
###--------------------------------------------------------------------
fit = lm(Y ~ . , data=rbind(train,val))
lm.pred = predict(fit, newdata = test[,-1])
## IS
lm.pred.IS = predict(fit, newdata = full.train[,-1])
lm.mse.IS = mean((lm.pred.IS - full.train$Y)^2)
lm.R2.IS = (1 - lm.mse.IS/(tot.test))*100
## What variables are in the oracle
fml = as.formula(str_c("Y ~ " , "X1 + ", "X2 + ", "X", Pc+3 ))
fit.oracle = lm(fml, data=rbind(train,val) )
oracle.pred = predict(fit.oracle, newdata = test[,-1])
oracle.pred.IS = predict(fit.oracle, newdata = full.train[,-1])
### ------------------------------------------------------------------
# Shrinking / selection parameter framework
###--------------------------------------------------------------------
### ------------------------------------------------------------------
#Ridge regression
##--------------------------------------------------------------------
ridge = glmnet(as.matrix(train[,-1]), as.matrix(train[,1]),
family = "gaussian", alpha=0 , lambda = lambda.grid)
pre.ridge.val = predict(ridge , newx = as.matrix(val[,-1]))
mse.ridge.val = numeric(length(pre.ridge.val[1,]))
for (i  in 1:length(pre.ridge.val[1,])){
mse.ridge.val[i] = mean((pre.ridge.val[,i] - val[,1])^2)
}
min.lambda.ridge = which.min(mse.ridge.val) %>% ridge$lambda[.]
ridge.pred = predict(ridge, newx=as.matrix(test[,-1]),
s = min.lambda.ridge)
ridge.pred.IS = predict(ridge, newx = as.matrix(full.train[,-1]),
s = min.lambda.ridge)
### ------------------------------------------------------------------
#Lasso regression
##--------------------------------------------------------------------
lasso = glmnet(as.matrix(train[,-1]), as.matrix(train[,1]),
family = "gaussian", alpha=1, lambda = lambda.grid)
#x
#plot(x)
#coef(x)[,45]
pre.lasso.val = predict(lasso , newx = as.matrix(val[,-1]), type = "link" )
mse.lasso.val = numeric(length(pre.lasso.val[1,]))
for (i  in 1:length(pre.lasso.val[1,])){
mse.lasso.val[i] = mean((pre.lasso.val[,i] - val[,1])^2)
}
min.lambda.lasso = which.min(mse.lasso.val) %>% lasso$lambda[.]
lasso.pred = predict(lasso, newx = as.matrix(test[,-1]),
s = min.lambda.lasso)
lasso.pred.IS = predict(lasso, newx = as.matrix(full.train[,-1]),
s = min.lambda.lasso)
### ------------------------------------------------------------------
#Parameter search for alpha and lambda in elastic net.
##--------------------------------------------------------------------
al <- seq(0,1,length.out = 5)
lol = lapply(al,
function(i) glmnet(as.matrix(train[,-1]), as.matrix(train[,1]),
alpha=i, lambda = lambda.grid))
what = lapply(lol  ,
function(i) predict(i, newx=as.matrix(val[,-1])))
what2 = sapply(what, function(i) colMeans((i - val[,1])^2) %>%
which.min(.) )
mse.enet.vect = numeric(length(al))
for (i in 1:length(al)){
fit = lol[[i]]$lambda[what2[[i]]]
mse.enet.vect[i] = mean((predict(lol[[i]], s=fit, newx=as.matrix(val[,-1])) - val[,1])^2)
}
which.model =which.min(mse.enet.vect)
enet.alpha = which.model %>% al[.]
enet.lambda = lol[[which.model]]$lambda[ what2[[which.model]] ]
##calculate the mse on the test sample ...
glmnet.model = glmnet(as.matrix(train[,-1]), as.matrix(train[,1]),
family = "gaussian",
alpha=enet.alpha , lambda = enet.lambda)
enet.pred = predict(glmnet.model, newx = as.matrix(test[,-1]))
enet.pred.IS = predict(glmnet.model, newx = as.matrix(full.train[,-1]))
### ------------------------------------------------------------------
#Parameter search PLS and PCR. Predictor averaging techniques.
##--------------------------------------------------------------------
pcr.fit = pcr(Y ~ . , data = train ,  scale = TRUE,
validation = "none")
mse.pcr = numeric(pcr.fit$ncomp)
for (i in 1:pcr.fit$ncomp){
pcr.pred.val = predict(pcr.fit, val[,-1], ncomp = i)
mse.pcr[i] = mean((pcr.pred.val - val[,1])^2)
}
pcr.comp = which.min(mse.pcr)
pcr.pred = predict(pcr.fit, test[,-1], ncomp = pcr.comp)
pcr.pred.IS = predict(pcr.fit, full.train[,-1], ncomp = pcr.comp)
plsr.fit = plsr(Y ~ . , data = train, scale = TRUE,
validation = "none")
mse.plsr.val = numeric(plsr.fit$ncomp)
for (i in 1:plsr.fit$ncomp){
plsr.pred.val = predict(plsr.fit, val[,-1], ncomp = i)
mse.plsr.val[i] = mean((plsr.pred.val - val[,1])^2)
}
plsr.comp = which.min(mse.plsr.val)
plsr.pred = predict(plsr.fit , test[,-1], ncomp = plsr.comp)
plsr.pred.IS = predict(plsr.fit, full.train[,-1], ncomp = pcr.comp)
### ------------------------------------------------------------------
#Generalized linear model (GAM) penalized by group lasso.
##--------------------------------------------------------------------
num.pred = length(train[1,-1])
lambda.grid.glm = 10^seq(3,-7, length = 100)
predictor.matrix.train = lapply(2:10, function(x) lapply(seq(2,num.pred+1),
function(i) bs(train[,i], degree = 2, df = x)) %>%
do.call(cbind, .) )
predictor.matrix.val = lapply(2:10, function(x) lapply(seq(2,num.pred+1),
function(i) bs(val[,i], degree = 2, df = x)) %>%
do.call(cbind, .) )
predictor.matrix.test = lapply(2:10, function(x) lapply(seq(2,num.pred+1),
function(i) bs(test[,i], degree = 2, df = x)) %>%
do.call(cbind, .) )
predictor.matrix.fulltrain = lapply(2:10, function(x) lapply(seq(2,num.pred+1),
function(i) bs(full.train[,i], degree = 2, df = x)) %>%
do.call(cbind, .) )
groups = lapply(2:10, function(i) rep(1:num.pred, each = i
) )
## TRain Models
glm.models = lapply(1:9, function(i) gglasso(predictor.matrix.train[[i]],
as.matrix(train[,1]),
group = groups[[i]],
loss = "ls", lambda = lambda.grid.glm) )
#plot(glm.models[[9]])
#coef(glm.models[[9]], s = lambda.grid.glm[4])
## Which lambda minimizes the mse for each of the models with different df (no. of knots)
which.lambda.glm =   sapply(1:length(glm.models)  ,
function(i) predict(glm.models[[i]], newx=predictor.matrix.val[[i]],type="link") %>%
colMeans((. - val[,1])^2) %>%
which.min() )
#glm.models[[i]]$lambda[which.lambda.glm[[i]]]
## Evaluate on  validation set
glm.mse.val = numeric(length(glm.models))
for (i in 1:length(glm.models)){
glm.pred.val = predict(glm.models[[i]],
newx=predictor.matrix.val[[i]], type="link",
s = glm.models[[i]]$lambda[which.lambda.glm[[i]]])
glm.mse.val[i] = mean((glm.pred.val - val[,1])^2)
}
#plot(glm.mse.val)
best.model = which.min(glm.mse.val)
glm.pred = predict(glm.models[[best.model]], newx=predictor.matrix.test[[best.model]], type="link",
s = glm.models[[best.model]]$lambda[which.lambda.glm[[best.model]]] )
glm.pred.IS = predict(glm.models[[best.model]],
newx=predictor.matrix.fulltrain[[best.model]],
type="link",
s = glm.models[[best.model]]$lambda[which.lambda.glm[[best.model]]] )
### ------------------------------------------------------------------
#Tree based models (boosting and bagging).
##--------------------------------------------------------------------
### Random Forest
library(ranger)
no.pred = dim(train)[2] - 1
RF_grid <- expand.grid(
mtry       = c(floor(sqrt(no.pred)), floor(no.pred/3)),
node_size  = c(10, 500, 1000 ),
OOB_RMSE  = 0
)
## Use all data as we can use OOB error to evaluate tuning parameters
train.rf = rbind(train,val)
for(i in 1:nrow(RF_grid)) {
# train model
model <- ranger(
formula         = Y ~ .,
data            = train.rf,
num.trees       = 500,
mtry            = RF_grid$mtry[i],
min.node.size   = RF_grid$node_size[i]
)
# add OOB error to grid
RF_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
best.model.RF = which.min(RF_grid$OOB_RMSE) %>% RF_grid[.,]
model.RF = ranger(
formula         = Y ~ .,
data            = train.rf,
num.trees       = 500,
mtry            = best.model.RF$mtry,
min.node.size   = best.model.RF$node_size
)
randomF.pred = predict(model.RF, test[,-1])$predictions
randomF.pred.IS = predict(model.RF, full.train[,-1])$predictions
## GRadient Boosted regression trees
library(gbm)
boost_grid <- expand.grid(
depth       = seq(1,4),
shrink  = c(0.01, 0.2)
)
boost_grid_list = apply(boost_grid, MARGIN = 1, as.list)
model.boosting = lapply(boost_grid_list, function(x)
gbm(Y~. , distribution = "gaussian", data = train,
interaction.depth = x$depth,
shrinkage = x$shrink,
n.trees =500 ) )
boost.pred.val = lapply(model.boosting ,
function(x) predict(x, newdata=val[,-1], n.trees=500))
boost.pred.mse = sapply(boost.pred.val, function(x) mean(( x - val$Y)^2) )
opt.boost = which.min(boost.pred.mse)
boost.pred = predict(model.boosting[[opt.boost]], newdata = test[,-1], n.trees =500)
boost.pred.IS = predict(model.boosting[[opt.boost]], newdata = full.train[,-1], n.trees =500)
## In sample numbers
lm.mse.IS = mean((lm.pred.IS - full.train$Y)^2)
lm.R2.IS = (1 - lm.mse.IS/(tot.test))*100
oracle.mse.IS = mean((oracle.pred.IS - full.train$Y)^2)
oracle.R2.IS = (1 - oracle.mse.IS/(tot.test))*100
ridge.mse.IS = mean((ridge.pred.IS - full.train$Y)^2)
ridge.R2.IS = (1 - ridge.mse.IS/(tot.test))*100
lasso.mse.IS = mean((lasso.pred.IS - full.train$Y)^2)
lasso.R2.IS = (1 - lasso.mse.IS/(tot.test))*100
enet.mse.IS = mean((enet.pred.IS - full.train$Y)^2)
enet.R2.IS = (1 - enet.mse.IS/(tot.test))*100
plsr.mse.IS = mean((plsr.pred.IS - full.train$Y)^2)
plsr.R2.IS = (1 - plsr.mse.IS/(tot.test))*100
pcr.mse.IS = mean((pcr.pred.IS - full.train$Y)^2)
pcr.R2.IS = (1 - pcr.mse.IS/(tot.test))*100
glm.mse.IS = mean((glm.pred.IS - full.train$Y)^2)
glm.R2.IS = (1 - glm.mse.IS/(tot.test))*100
randomF.mse.IS = mean((randomF.pred.IS - full.train$Y)^2)
randomF.R2.IS = (1 - randomF.mse.IS/(tot.test))*100
boost.mse.IS = mean((boost.pred.IS - full.train$Y)^2)
boost.R2.IS = (1 - boost.mse.IS/(tot.test))*100
#######
# Calculation of MSE and R^2 for all model based on predictions from loop
#######
lm.mse = mean((lm.pred - df$Y[test.sample])^2)
lm.R2 = (1 - lm.mse/(tot.test))*100
oracle.mse = mean((oracle.pred - df$Y[test.sample])^2)
oracle.R2 = (1 - oracle.mse/(tot.test))*100
ridge.mse = mean((ridge.pred - df$Y[test.sample])^2)
ridge.R2 = (1 - ridge.mse/(tot.test) )*100
lasso.mse = mean((lasso.pred - df$Y[test.sample])^2)
lasso.R2 = (1 - lasso.mse/(tot.test) )*100
enet.mse = mean((enet.pred - df$Y[test.sample])^2)
enet.R2 = (1 - enet.mse/(tot.test) )*100
pcr.mse = mean((pcr.pred - df$Y[test.sample])^2)
pcr.R2 = (1 - pcr.mse/(tot.test) )*100
plsr.mse = mean((plsr.pred - df$Y[test.sample])^2)
plsr.R2 = (1 - plsr.mse/(tot.test) )*100
glm.mse = mean((glm.pred - df$Y[test.sample])^2)
glm.R2 = (1 - glm.mse/(tot.test) )*100
randomF.mse = mean((randomF.pred - df$Y[test.sample])^2)
randomF.R2 = (1 - randomF.mse/(tot.test) )*100
boost.mse = mean((boost.pred - df$Y[test.sample])^2)
boost.R2 = (1 - boost.mse/(tot.test) )*100
#### DIEBOLD MARIANO
test.sample.dates = seq(T - val_T +1, T)
make.pred.full.vector = length(df$Y)-length(lm.pred)
DB = function(pred1, pred2){
pred1 = c(rep(0, make.pred.full.vector), pred1)
pred2 = c(rep(0, make.pred.full.vector), pred2)
d = numeric(length(test.sample.dates))
for (i in 1:length(test.sample.dates)){
t = test.sample.dates[i]
obs = which(df$time==t)
e1 = (pred1[obs] - df$Y[obs])^2
e2 = (pred2[obs] - df$Y[obs])^2
d[i] = mean(e1 - e2)
}
if(norm(d, type = "2") != 0){
fit = lm(d ~ 1)
vcovHAC(fit)
test_stat = mean(d)/sqrt(vcovHAC(fit))
} else {test_stat = 0}
return( test_stat )
}
test.predictions = list("OLS" = lm.pred, "Ridge" = ridge.pred,
"Lasso" = lasso.pred, "ENET" = enet.pred, "PCR"=pcr.pred,
"PLSR"=plsr.pred, "GLM" = glm.pred, "RandomF" = randomF.pred,
"GBRT" = boost.pred,
"Oracle"=oracle.pred)
DB.grid = c("OLS" = 1, "Ridge" = 2,
"Lasso" = 3, "ENET" = 4, "PCR"=5,
"PLSR"=6, "GLM" = 7, "RandomF" = 8,
"GBRT" = 9,
"Oracle"=10) %>% expand.grid(.,.)
DB.grid = apply(DB.grid, MARGIN = 1, as.list)
DB.grid = lapply(DB.grid,
function(x) list(test.predictions[[x$Var1]],
test.predictions[[x$Var2]]))
Marianos = sapply(DB.grid, function(x) DB(x[[1]], x[[2]]))
##################################################################
########### ------- END OF EXCUTE ---------------- ###########
##################################################################
return(list("OLS"=c("R2-OOS"=lm.R2, "R2-IS"=lm.R2.IS, "MSE-OOS"=lm.mse, "MSE-IS"=lm.mse.IS),
"Ridge"= c("R2-OOS"=ridge.R2, "R2-IS"=ridge.R2.IS, "MSE-OOS"=ridge.mse, "MSE-IS"=ridge.mse.IS),
"Lasso"= c("R2-OOS"=lasso.R2, "R2-IS"=lasso.R2.IS, "MSE-OOS"=lasso.mse, "MSE-IS"=lasso.mse.IS ),
"ENet" = c("R2-OOS"=enet.R2, "R2-IS"=enet.R2.IS, "MSE-OOS"=enet.mse, "MSE-IS"=enet.mse.IS ),
"PCR" = c("R2-OOS"=pcr.R2, "R2-IS"=pcr.R2.IS, "MSE-OOS"=pcr.mse, "MSE-IS"=pcr.mse.IS ),
"PLSR" = c("R2-OOS"=plsr.R2, "R2-IS"=plsr.R2.IS, "MSE-OOS"=plsr.mse, "MSE-IS"=plsr.mse.IS ),
"GLM" = c("R2-OOS"=glm.R2, "R2-IS"=glm.R2.IS, "MSE-OOS"=glm.mse, "MSE-IS"=glm.mse.IS ),
"RandomF"= c("R2-OOS"=randomF.R2, "R2-IS"=randomF.R2.IS, "MSE-OOS"=randomF.mse, "MSE-IS"=randomF.mse.IS ),
"GBRT" = c("R2-OOS"=boost.R2, "R2-IS"=boost.R2.IS, "MSE-OOS"=boost.mse, "MSE-IS"=boost.mse.IS ),
"Oracle" = c("R2-OOS"=oracle.R2, "R2-IS"=oracle.R2.IS, "MSE-OOS"=oracle.mse, "MSE-IS"=oracle.mse.IS ),
"Diebold Mariano test" = Marianos
))
}
test = execute(DGP = "b")
load("/Users/alexanderbech/Dropbox/Project/Neural.RData")
sapply(1:50, function(i) MC.a.100[[i]]$NN3[1:2])
sapply(1:50, function(i) MC.a.100[[i]]$NN3[1:2]) %>% t()
sapply(1:50, function(i) MC.a.100[[i]]$NN3[1:2]) %>% t() %>% colMeans()
##NEural Tidying
#Simulation manipulatio
load("/Users/alexanderbech/Dropbox/Project/Neural.RData")
collect.MC.a.50 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.a.50[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.b.50 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.b.50[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.a.100 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.a.100[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.b.100 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.b.100[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
MC_table = data.frame(
"$R^2(%)$" = names(MC.a.50[[1]][1:13]),
"IS" = collect.MC.a.50[2,],
"OOS" = collect.MC.a.50[1,],
"IS" = collect.MC.a.100[2,],
"OOS" = collect.MC.a.100[1,],
"IS" = collect.MC.b.50[2,],
"OOS" = collect.MC.b.50[1,],
"IS" = collect.MC.b.100[2,],
"OOS" = collect.MC.b.100[1,]
)
names(MC.a.50[[1]][1:13]
names(MC.a.50[[1]][1:13])
names(MC.a.50[[1]][1:2])
##NEural Tidying
#Simulation manipulatio
load("/Users/alexanderbech/Dropbox/Project/Neural.RData")
collect.MC.a.50 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.a.50[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.b.50 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.b.50[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.a.100 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.a.100[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.b.100 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.b.100[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
MC_table = data.frame(
"$R^2(%)$" = names(MC.a.50[[1]][1:2]),
"IS" = collect.MC.a.50[2,],
"OOS" = collect.MC.a.50[1,],
"IS" = collect.MC.a.100[2,],
"OOS" = collect.MC.a.100[1,],
"IS" = collect.MC.b.50[2,],
"OOS" = collect.MC.b.50[1,],
"IS" = collect.MC.b.100[2,],
"OOS" = collect.MC.b.100[1,]
)
##NEural Tidying
#Simulation manipulatio
load("/Users/alexanderbech/Dropbox/Project/Neural.RData")
collect.MC.a.50 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.a.50[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.b.50 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.b.50[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.a.100 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.a.100[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
collect.MC.b.100 = sapply(1:2,
function(j) sapply(1:50, function(i) MC.b.100[[i]][[j]][1:2]) %>%
t()  %>% #ifelse(.>(-30), ., NA) %>%
colMeans(na.rm = TRUE) )
MC_table = data.frame(
"$R^2(%)$" = names(MC.a.50[[1]][1:2]),
"IS" = collect.MC.a.50[2,],
"OOS" = collect.MC.a.50[1,],
"IS" = collect.MC.a.100[2,],
"OOS" = collect.MC.a.100[1,],
"IS" = collect.MC.b.50[2,],
"OOS" = collect.MC.b.50[1,],
"IS" = collect.MC.b.100[2,],
"OOS" = collect.MC.b.100[1,]
)
MC_table
print(xtable(MC_table, type = "latex"), file = "Neural.tex")
library(xtable)
print(xtable(MC_table, type = "latex"), file = "Neural.tex")
load("/Users/alexanderbech/Dropbox/Project/diebold_sim.RData")
load("/Users/alexanderbech/Dropbox/Project/diebold_sim.RData")
MC.a.100[[1]]$`Diebold Mariano test`
MC.a.100[[1]]$`Diebold Mariano test` %>% length()
c("OLS" = 1, "Ridge" = 2,
"Lasso" = 3, "ENET" = 4, "PCR"=5,
"PLSR"=6, "GLM" = 7, "RandomF" = 8,
"GBRT" = 9,
"Oracle"=10) %>% expand.grid(.,.) %>% dim()
MC.a.100[[1]]$`Diebold Mariano test` %>% matrix(., nrow = 10)
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`)
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% class()
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% dim()
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% rowMeans()
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% rowMeans() %>% dim()
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% rowMeans() %>% length()
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% rowMeans()
sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>% rowMeans() %>% matrix(.,nrow = 10)
## Diebold Mariano simulations
DB.a.50 = sapply(1:50, function(i) MC.a.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.a.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.50 = sapply(1:50, function(i) MC.b.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.100
## Diebold Mariano simulations
DB.a.50 = sapply(1:50, function(i) MC.a.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.a.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.50 = sapply(1:50, function(i) MC.b.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.grid = c("OLS" = 1, "Ridge" = 2,
"Lasso" = 3, "ENET" = 4, "PCR"=5,
"PLSR"=6, "GLM" = 7, "RandomF" = 8,
"GBRT" = 9,
"Oracle"=10)
names(DB.grid)
## Diebold Mariano simulations
DB.a.50 = sapply(1:50, function(i) MC.a.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.a.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.50 = sapply(1:50, function(i) MC.b.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.b.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10) %>% as_tibble()
DB.grid = c("OLS" = 1, "Ridge" = 2,
"Lasso" = 3, "ENET" = 4, "PCR"=5,
"PLSR"=6, "GLM" = 7, "RandomF" = 8,
"GBRT" = 9,
"Oracle"=10)
colnames(DB.a.50) = names(DB.grid)
rownames(DB.a.50) = names(DB.grid)
colnames(DB.a.100) = names(DB.grid)
rownames(DB.a.100) = names(DB.grid)
colnames(DB.b.50) = names(DB.grid)
rownames(DB.b.50) = names(DB.grid)
colnames(DB.b.100) = names(DB.grid)
rownames(DB.b.100) = names(DB.grid)
DB.a.50
DB.a.50 = sapply(1:50, function(i) MC.a.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10)
DB.grid = c("OLS" = 1, "Ridge" = 2,
"Lasso" = 3, "ENET" = 4, "PCR"=5,
"PLSR"=6, "GLM" = 7, "RandomF" = 8,
"GBRT" = 9,
"Oracle"=10)
colnames(DB.a.50) = names(DB.grid)
rownames(DB.a.50) = names(DB.grid)
DB.a.50
DB.a.50 %>% class()
print(xtable(DB.a.50, type = "latex"), file = "DB.a.50.tex")
getwd()
## Diebold Mariano simulations
DB.a.50 = sapply(1:50, function(i) MC.a.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10)
DB.a.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10)
DB.b.50 = sapply(1:50, function(i) MC.b.50[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10)
DB.b.100 = sapply(1:50, function(i) MC.a.100[[i]]$`Diebold Mariano test`) %>%
rowMeans() %>% matrix(.,nrow = 10)
DB.grid = c("OLS" = 1, "Ridge" = 2,
"Lasso" = 3, "ENET" = 4, "PCR"=5,
"PLSR"=6, "GLM" = 7, "RandomF" = 8,
"GBRT" = 9,
"Oracle"=10)
colnames(DB.a.50) = names(DB.grid)
rownames(DB.a.50) = names(DB.grid)
colnames(DB.a.100) = names(DB.grid)
rownames(DB.a.100) = names(DB.grid)
colnames(DB.b.50) = names(DB.grid)
rownames(DB.b.50) = names(DB.grid)
colnames(DB.b.100) = names(DB.grid)
rownames(DB.b.100) = names(DB.grid)
print(xtable(DB.a.50, type = "latex"), file = "DB.a.50.tex")
print(xtable(DB.a.100, type = "latex"), file = "DB.a.100.tex")
print(xtable(DB.b.50, type = "latex"), file = "DB.b.50.tex")
print(xtable(DB.b.100, type = "latex"), file = "DB.b.100.tex")
